#
# Copyright 2020 Apple Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


from collections import defaultdict
from dataclasses import dataclass


from ._protocols import DimensionReductionStrategyType
from . import _reducers
from dnikit.exceptions import DNIKitException
import dnikit.typing._types as t
from dnikit.base import (
    Batch,
    Introspector,
    Producer,
    PipelineStage
)

_LayerStrategies = t.Mapping[str, DimensionReductionStrategyType]
OneOrManyDimStrategies = t.Union[DimensionReductionStrategyType, _LayerStrategies]


@t.final
@dataclass(frozen=True)
class DimensionReduction(Introspector, PipelineStage):
    """
    :class:`Introspector <dnikit.base.Introspector>` to reduce dimensionality of
    :class`Batch <dnikit.base.Batch>` :attr:`fields <dnikit.base.Batch.fields>` (usually
    model responses).

    Like other :class:`introspectors <dnikit.base.Introspector>`, use
    :func:`DimensionReduction.introspect <introspect>` to instantiate.
    """

    _reducers: t.Mapping[str, DimensionReductionStrategyType]
    """Mapping of response names to fitted :class:`DimensionReductionStrategyType` objects"""

    @t.final
    class Strategy:
        """
        Bundled dimension reduction strategies.  See :class:`DimensionReductionStrategyType`.

        The available options are:

        - :attr:`PCA` -- an Incremental PCA algorithm from ``sklearn`` that can process data
          incrementally without accumulating the dataset
        - :attr:`StandardPCA` -- PCA algorithm from ``sklearn`` that requires accumulating the full
          dataset in memory
        - :attr:`TSNE` -- t-SNE algorithm from ``sklearn`` that requires accumulating the full
          dataset in memory
        - :attr:`UMAP` -- the UMAP algorithm from ``umap-learn`` that requires accumulating the
          full dataset in memory
        - :attr:`PaCMAP` -- the PaCMAP algorithm that requires accumulating the full
          dataset in memory
        """
        PCA: t.Final = _reducers.PCA
        StandardPCA: t.Final = _reducers.StandardPCA
        UMAP: t.Final = _reducers.UMAP
        TSNE: t.Final = _reducers.TSNE
        PaCMAP: t.Final = _reducers.PaCMAP

    @staticmethod
    def introspect(producer: Producer, *,
                   strategies: OneOrManyDimStrategies,
                   batch_size: t.Optional[int] = None) -> "DimensionReduction":
        """
        Perform dimension reduction using training data generated by ``producer``, and
        return a ``DimensionReduction`` :class:`PipelineStage <dnikit.base.PipelineStage>` that can
        perform dimensionality reduction in a :func:`pipeline <dnikit.base.pipeline>`.

        The ``producer`` must produce 1d vectors, e.g. the :class:`Batch <dnikit.base.Batch>` will
        be of dimension ``BxN``.  See :class:`Flattener <dnikit.processors.Flattener>` or
        :class:`Pooler <dnikit.processors.Pooler>` if multi-dimensional data is used.

        Note: some strategies will need to read all of the response data into memory to fit
        their model.  Currently only the
        :attr:`PCA <dnikit.introspectors.DimensionReduction.Strategy.PCA>` algorithm runs
        in a streaming fashion.

        Args:
            producer: the source of data to train the ``strategies`` on
            strategies: **[keyword arg]** which dimension reduction
                :class:`strategy <dnikit.introspectors.DimensionReduction.Strategy>` to use or
                a mapping from :attr:`field <dnikit.base.Batch.fields>` name to
                :class:`strategy <dnikit.introspectors.DimensionReduction.Strategy>`
                (for running a different dimension reduction per layer.
            batch_size: **[keyword arg, optional]** size of batch to read out -- this must be >=
                the target dimension.  For some strategies like
                :attr:`PCA <dnikit.introspectors.DimensionReduction.Strategy.PCA>`,
                this will improve the quality of the dimension reduction.
                The default value will select the ``batch_size`` automatically.

        Raises:
            DNIKitException: if a layer's response shape does not have exactly 2 dimensions.
            DNIKitException: if the ``batch_size`` is smaller than the target dimensions.
        """

        # convert the strategies into reducers (t.Mapping[str, DimensionReductionStrategyType])
        if isinstance(strategies, DimensionReductionStrategyType):
            # if only one was given, add a factory method to
            # defaultdict that will copy it for all names
            single_strategy = strategies  # Prevents mypy from complaining about Union
            reducers: _LayerStrategies = defaultdict(lambda: single_strategy._clone())
            all_keys = True

            if batch_size is None:
                # this method didn't always take a batch size -- compute one
                # for backward compatibility
                batch_size = single_strategy.default_batch_size()
            else:
                single_strategy.check_batch_size(batch_size)

        else:
            # given per layer, convert the factories into instances
            reducers = strategies
            all_keys = False

            if batch_size is None:
                batch_size = max([
                    reducer.default_batch_size()
                    for reducer in reducers.values()
                ])
            else:
                for reducer in reducers.values():
                    reducer.check_batch_size(batch_size)

        # fit the data from the producer
        for batch in producer(batch_size):
            for name, field in batch.fields.items():
                if all_keys or name in reducers:
                    if len(field.shape) != 2:
                        raise DNIKitException(
                            f'Unable to reduce response of shape {field.shape}. '
                            f'The shape is expected to have 2 dimensions'
                        )
                    reducers[name].fit_incremental(field)

        for reducer in reducers.values():
            reducer.fit_complete()

        return DimensionReduction(_reducers=reducers)

    def _get_batch_processor(self) -> t.Callable[[Batch], Batch]:
        # not used -- see _pipeline()
        raise NotImplementedError()

    def _pipeline(self, producer: Producer) -> Producer:
        # override _pipeline to do custom processing -- some of the reducers
        # may consume their input and produce their output in one shot.  this
        # can be merged with other reducers that transform data in a streaming
        # fashion.

        # for any reducers that work in one shot, collect those first
        transformed = {}
        for name, reducer in self._reducers.items():
            if reducer.is_one_shot:
                transformed[name] = reducer.transform_one_shot()

        # this is roughly the replacement for _get_batch_processor() but it captures
        # the `transformed` state reflected in the preceding line.
        def new_producer(batch_size: int) -> t.Iterable[Batch]:
            start = 0
            for batch in producer(batch_size):
                end = start + batch.batch_size
                builder = Batch.Builder(base=batch)
                for name, reducer in self._reducers.items():
                    if reducer.is_one_shot:
                        builder.fields[name] = transformed[name][start:end]
                    else:
                        builder.fields[name] = reducer.transform(batch.fields[name])
                start = end
                yield builder.make_batch()

        return new_producer
